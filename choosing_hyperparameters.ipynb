{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CROSS VALIDATION FOR DIFFERENT MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossvalidation import *\n",
    "from implementations import *\n",
    "from feature_engineering import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading train data\n",
    "\n",
    "filename = 'train.csv'\n",
    "data_folder = './data/'\n",
    "file_path = data_folder + filename\n",
    "y,tx,ids,features = load_train_data(file_path)\n",
    "\n",
    "# Computing preprocessing routine\n",
    "\n",
    "list_subsets, list_features, y_0, y_1, y_2_3, columns_to_drop_in_subsets = preprocessing(tx,y,ids,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to introduce interaction factors between variables during the preprocessing routine. \n",
    "However, there is not statistical significance that multiplying variables with trigonometric functions \n",
    "may improve the model performance. Therefore, since trigonometric values are the last columns of each \n",
    "dataset, we save in a list how many columns are related to trigonometric values in each subset in order \n",
    "not to multiply columns with them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_trig_features=[2,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining parameters to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-7,-3,5)\n",
    "degrees = [3,5,7]\n",
    "k_fold = 4\n",
    "gamma = 0.1\n",
    "max_iters = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining lists to save optimal degrees and lambdas for each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lambdas = [0]*3\n",
    "optimal_degrees = [1]*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing cross validation on subsets_0 for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimal_degrees[\u001b[38;5;241m0\u001b[39m], optimal_lambdas[\u001b[38;5;241m0\u001b[39m], best_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation_demo_ridge\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_subsets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                                                               \u001b[49m\u001b[43mhow_many_trig_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\MY_ML_PROJECT\\crossvalidation.py:184\u001b[0m, in \u001b[0;36mcross_validation_demo_ridge\u001b[1;34m(y, tx, k_fold, lambdas, degrees, no_interaction_factors, seed)\u001b[0m\n\u001b[0;32m    182\u001b[0m l_te \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k_fold):\n\u001b[1;32m--> 184\u001b[0m     loss_tr, loss_te \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation_ridge\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mno_interaction_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m     l_te \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_te\n\u001b[0;32m    186\u001b[0m     l_tr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_tr\n",
      "File \u001b[1;32m~\\Desktop\\MY_ML_PROJECT\\crossvalidation.py:141\u001b[0m, in \u001b[0;36mcross_validation_ridge\u001b[1;34m(y, x, k_indices, k, lambda_, degree, no_interaction_factors)\u001b[0m\n\u001b[0;32m    137\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y[test_indices]\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# We compute polynomial expansion (and automatically add the offset column)\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m poly_train \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_poly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\u001b[43mno_interaction_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m poly_test \u001b[38;5;241m=\u001b[39m build_poly(x_test, degree, no_interaction_factors)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Finding optimal weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\MY_ML_PROJECT\\preprocessing.py:132\u001b[0m, in \u001b[0;36mbuild_poly\u001b[1;34m(x, degree, no_interaction_factors_columns)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,len_without_offset_and_expansion\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,len_without_offset_and_expansion):\n\u001b[1;32m--> 132\u001b[0m         phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mc_[phi, \u001b[43mphi\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mphi\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m]\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m phi\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimal_degrees[0], optimal_lambdas[0], best_rmse = cross_validation_demo_ridge(y_0, list_subsets[0], k_fold, lambdas, degrees,\n",
    "                                                                               how_many_trig_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Doing cross validation on subsets_1 for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test rmse is 0.00010 with a test rmse of 0.371. The best degree is 7.0\n"
     ]
    }
   ],
   "source": [
    "optimal_degrees[1], optimal_lambdas[1], best_rmse = cross_validation_demo_ridge(y_1, list_subsets[1], k_fold, lambdas, degrees,\n",
    "                                                                               how_many_trig_features[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Doing cross validation on subsets_2_3 for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test rmse is 0.00010 with a test rmse of 0.347. The best degree is 7.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimal_degrees' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43moptimal_degrees\u001b[49m[\u001b[38;5;241m2\u001b[39m], optimal_lambdas[\u001b[38;5;241m2\u001b[39m], best_rmse \u001b[38;5;241m=\u001b[39m cross_validation_demo_ridge(y_2_3, list_subsets[\u001b[38;5;241m2\u001b[39m], k_fold, lambdas, degrees,\n\u001b[0;32m      2\u001b[0m                                                                                how_many_trig_features[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimal_degrees' is not defined"
     ]
    }
   ],
   "source": [
    "optimal_degrees[2], optimal_lambdas[2], best_rmse = cross_validation_demo_ridge(y_2_3, list_subsets[2], k_fold, lambdas, degrees,\n",
    "                                                                               how_many_trig_features[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGULARIZED LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing cross validation on subsets_0 for regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test logloss is 0.00000 with a test logloss of 0.362. The best degree is 3.0\n"
     ]
    }
   ],
   "source": [
    "best_degree,best_lambda,_ = cross_validation_demo_log(y_0, list_subsets[0], k_fold, lambdas, gamma, max_iters,degrees, how_many_trig_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Doing cross validation on subsets_1 for regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test logloss is 0.00000 with a test logloss of 0.419. The best degree is 3.0\n"
     ]
    }
   ],
   "source": [
    "best_degree,best_lambda,_ = cross_validation_demo_log(y_1, list_subsets[1], k_fold, lambdas, gamma, max_iters,degrees, how_many_trig_features[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Doing cross validation on subsets_2_3 for regularized logistc regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test logloss is 0.00000 with a test logloss of 0.377. The best degree is 3.0\n"
     ]
    }
   ],
   "source": [
    "best_degree,best_lambda,_ = cross_validation_demo_log(y_2_3, list_subsets[2], k_fold, lambdas, gamma, max_iters,degrees, how_many_trig_features[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### COMPUTING ACCURACY FOR RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_outputs = [y_0,y_1,y_2_3]\n",
    "for idx in range(3):\n",
    "    list_subsets[idx] = list_subsets[idx][int(0.6*list_subsets[idx].shape[0]),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Computing accuracy for ridge regression using optimal values as hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train accuracy: 0.8362282705107522\n",
      "std train accuracy: 0.0005229317330015228\n",
      "Average test accuracy: 0.8349103493434902\n",
      "std train accuracy: 0.0008072756696673245\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(list_outputs,list_subsets,0.7,[0.00001,0.00001,0.00001],[7,7,7],[0,0,0],pred_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPUTING ACCURACY FOR REGULARIZED LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing accuracy for regularized logistic regression using optimal values as hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train accuracy: 0.8320193575709321\n",
      "std train accuracy: 0.00046693834276353334\n",
      "Average test accuracy: 0.8319370556540728\n",
      "std train accuracy: 0.0010278248657757754\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(list_outputs,list_subsets,0.7,[0.0000, 0.0000, 0.0000],[3,3,3],how_many_trig_features, pred_threshold=0.55,method = 'logistic',gamma = 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e9c25df0253b19710fd2eabc0119b804c820fc74e8ba938ebab686d76fc4dfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
