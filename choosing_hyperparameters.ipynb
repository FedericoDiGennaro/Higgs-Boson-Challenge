{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CROSS VALIDATION FOR DIFFERENT MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossvalidation import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following magic method runs feature_engineering.ipynb\n",
    "%run ./feature_engineering.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining parameters to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.logspace(-7,-3,5)\n",
    "degrees = [3,5,7]\n",
    "k_fold = 4\n",
    "gamma = 0.1\n",
    "max_iters = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining lists to save optimal degrees and lambdas for each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lambdas = [0]*3\n",
    "optimal_degrees = [1]*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing cross validation on subsets_0 for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimal_degrees[\u001b[38;5;241m0\u001b[39m], optimal_lambdas[\u001b[38;5;241m0\u001b[39m], best_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation_demo_ridge\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_subsets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegrees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                                                               \u001b[49m\u001b[43mhow_many_trig_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\MY_ML_PROJECT\\crossvalidation.py:184\u001b[0m, in \u001b[0;36mcross_validation_demo_ridge\u001b[1;34m(y, tx, k_fold, lambdas, degrees, no_interaction_factors, seed)\u001b[0m\n\u001b[0;32m    182\u001b[0m l_te \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k_fold):\n\u001b[1;32m--> 184\u001b[0m     loss_tr, loss_te \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation_ridge\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mno_interaction_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m     l_te \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_te\n\u001b[0;32m    186\u001b[0m     l_tr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_tr\n",
      "File \u001b[1;32m~\\Desktop\\MY_ML_PROJECT\\crossvalidation.py:141\u001b[0m, in \u001b[0;36mcross_validation_ridge\u001b[1;34m(y, x, k_indices, k, lambda_, degree, no_interaction_factors)\u001b[0m\n\u001b[0;32m    137\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y[test_indices]\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# We compute polynomial expansion (and automatically add the offset column)\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m poly_train \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_poly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\u001b[43mno_interaction_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m poly_test \u001b[38;5;241m=\u001b[39m build_poly(x_test, degree, no_interaction_factors)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Finding optimal weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\MY_ML_PROJECT\\preprocessing.py:132\u001b[0m, in \u001b[0;36mbuild_poly\u001b[1;34m(x, degree, no_interaction_factors_columns)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,len_without_offset_and_expansion\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,len_without_offset_and_expansion):\n\u001b[1;32m--> 132\u001b[0m         phi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mc_[phi, \u001b[43mphi\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mphi\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m]\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m phi\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimal_degrees[0], optimal_lambdas[0], best_rmse = cross_validation_demo_ridge(y_0, list_subsets[0], k_fold, lambdas, degrees,\n",
    "                                                                               how_many_trig_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing cross validation on subsets_1 for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test rmse is 0.00010 with a test rmse of 0.371. The best degree is 7.0\n"
     ]
    }
   ],
   "source": [
    "optimal_degrees[1], optimal_lambdas[1], best_rmse = cross_validation_demo_ridge(y_1, list_subsets[1], k_fold, lambdas, degrees,\n",
    "                                                                               how_many_trig_features[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing cross validation on subsets_2_3 for ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test rmse is 0.00010 with a test rmse of 0.347. The best degree is 7.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimal_degrees' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43moptimal_degrees\u001b[49m[\u001b[38;5;241m2\u001b[39m], optimal_lambdas[\u001b[38;5;241m2\u001b[39m], best_rmse \u001b[38;5;241m=\u001b[39m cross_validation_demo_ridge(y_2_3, list_subsets[\u001b[38;5;241m2\u001b[39m], k_fold, lambdas, degrees,\n\u001b[0;32m      2\u001b[0m                                                                                how_many_trig_features[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimal_degrees' is not defined"
     ]
    }
   ],
   "source": [
    "optimal_degrees[2], optimal_lambdas[2], best_rmse = cross_validation_demo_ridge(y_2_3, list_subsets[2], k_fold, lambdas, degrees,\n",
    "                                                                               how_many_trig_features[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGULARIZED LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing cross validation on subsets_0 for regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test rmse is 0.00010 with a test rmse of 0.367. The best degree is 2.0\n"
     ]
    }
   ],
   "source": [
    "best_degree,best_lambda,_ = cross_validation_demo_log(y_0, list_subsets[0], k_fold, lambdas, gamma, max_iters,degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing cross validation on subsets_1 for regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test rmse is 0.00010 with a test rmse of 0.436. The best degree is 2.0\n"
     ]
    }
   ],
   "source": [
    "best_degree,best_lambda,_ = cross_validation_demo_log(y_1, list_subsets[1], k_fold, lambdas, gamma, max_iters,degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing cross validation on subsets_2_3 for regularized logistc regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The choice of lambda which leads to the best test rmse is 0.00000 with a test rmse of 0.530. The best degree is 1.0\n"
     ]
    }
   ],
   "source": [
    "best_degree,best_lambda,_ = cross_validation_demo_log(y_2_3, list_subsets[2], k_fold, lambdas, gamma, max_iters,degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPUTING ACCURACY FOR RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m list_outputs \u001b[38;5;241m=\u001b[39m [y_0,y_1,y_2_3]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     list_subsets[idx] \u001b[38;5;241m=\u001b[39m \u001b[43mlist_subsets\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlist_subsets\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "list_outputs = [y_0,y_1,y_2_3]\n",
    "for idx in range(3):\n",
    "    list_subsets[idx] = list_subsets[idx][0.6*list_subsets[idx].shape[0],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing accuracy for ridge regression using optimal values as hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train accuracy: 0.8355977907175343\n",
      "std train accuracy: 0.00045993678717979854\n",
      "Average test accuracy: 0.8346096126125726\n",
      "std train accuracy: 0.0009731703940924729\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(list_outputs,list_subsets,0.7,[0.0001,0.0001,0.0001],[5,7,7],how_many_trig_features,pred_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train accuracy: 0.8362282705107522\n",
      "std train accuracy: 0.0005229317330015228\n",
      "Average test accuracy: 0.8349103493434902\n",
      "std train accuracy: 0.0008072756696673245\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(list_outputs,list_subsets,0.7,[0.00001,0.00001,0.00001],[7,7,7],[0,0,0],pred_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPUTING ACCURACY FOR REGULARIZED LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing accuracy for regularized logistic regression using optimal values as hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train accuracy: 0.8231101003116843\n",
      "std train accuracy: 0.0003788928826751066\n",
      "Average test accuracy: 0.8232453196920337\n",
      "std train accuracy: 0.0009840028946414355\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(list_outputs,list_subsets,0.7,[0.00001]*3,[2,2,2],pred_threshold=0.55,method = 'logistic',gamma = 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e9c25df0253b19710fd2eabc0119b804c820fc74e8ba938ebab686d76fc4dfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
