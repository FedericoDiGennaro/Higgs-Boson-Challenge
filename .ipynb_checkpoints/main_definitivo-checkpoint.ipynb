{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing preprocessing routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# The following magic method runs feature_engineering.ipynb\n",
    "%run ./feature_engineering.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining optimal hyperparameters values (see notebook choosing_hyperparameters.ipynb for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambdas = [0.000001]*3\n",
    "best_degrees = [7,7,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING TEST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test.csv'\n",
    "data_folder = './data/'\n",
    "file_path = data_folder + filename\n",
    "tx_test,test_ids,features_test=load_test_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving logical masks to divide test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_column = np.where(features_test == 'PRI_jet_num')[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_0_test,mask_1_test,mask_2_3_test = divide_indices_in_subsets(tx_test,categorical_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing categorical column since it is now useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test = np.delete(tx_test,categorical_column,axis = 1)\n",
    "# since we delete the column in tx, we also delete the name of the categorical feature used to divide the dataset\n",
    "features_test = np.delete(features_test,categorical_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting test dataset, the output vector and ids w.r.t according to the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_0_test, ids_0_test = divide_test_dataset_in_subsets(tx_test,test_ids,mask_0_test)\n",
    "subset_1_test, ids_1_test = divide_test_dataset_in_subsets(tx_test,test_ids,mask_1_test)\n",
    "subset_2_3_test, ids_2_3_test = divide_test_dataset_in_subsets(tx_test,test_ids,mask_2_3_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a list containing each subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subsets_test = [subset_0_test,subset_1_test,subset_2_3_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list containing features for each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features_test = [features]*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns as done for train dataset and managing remaining missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(3):\n",
    "    list_subsets_test[idx] = list_subsets_test[idx][:,columns_to_drop_in_subsets[idx]]\n",
    "    list_features_test[idx] = list_features_test[idx][columns_to_drop_in_subsets[idx]]\n",
    "    for col in range(list_subsets_test[idx].shape[1]):\n",
    "        median = np.nanmedian(list_subsets_test[idx][:,col])\n",
    "        index = np.isnan(list_subsets_test[idx][:,col])\n",
    "        list_subsets_test[idx][index,col] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column in subset_0 is a zeros vector (see the documentation). Therefore, we drop it not to have problems when\n",
    "standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subsets_test[0] = np.delete(list_subsets_test[0],-1, 1)\n",
    "list_features_test[0] = np.delete(list_features_test[0],-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining trigonometric features (sine and cosine) starting from columns related to angle values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_angles_0 = [11, 14, 16]\n",
    "columns_angles_1 = [11, 14, 16, 20]\n",
    "columns_angles_2 = [15, 18, 20, 27]\n",
    "\n",
    "list_subsets_test[0],list_features_test[0] = trigonometrics(list_subsets_test[0],columns_angles_0,list_features_test[0])\n",
    "list_subsets_test[1],list_features_test[1] = trigonometrics(list_subsets_test[1],columns_angles_1,list_features_test[1])\n",
    "list_subsets_test[2],list_features_test[2] = trigonometrics(list_subsets_test[2],columns_angles_2,list_features_test[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying logarithmic transformation to skewed distributions in each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_log_c0 = [0,1,2,3,5,6,7,9,11,13,14] \n",
    "to_log_c1 = [0,1,2,3,5,6,7,9,11,13,14,15,17]\n",
    "to_log_c2 = [0,1,2,3,5,8,9,10,13,15,17,18,19,22,24]\n",
    "\n",
    "\n",
    "list_subsets_test[0][:,to_log_c0] =log_transform(list_subsets_test[0][:,to_log_c0])\n",
    "list_subsets_test[1][:,to_log_c1] = log_transform(list_subsets_test[1][:,to_log_c1])\n",
    "list_subsets_test[2][:,to_log_c2] = log_transform(list_subsets_test[2][:,to_log_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling outliers by replacing them with 5% or 95% percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(3):\n",
    "    list_subsets_test[idx] = capping_outliers(list_subsets_test[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns corresponding to useless variables in each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=list(range(list_subsets_test[0].shape[1]))\n",
    "useful_c0 = np.delete(a,useless_c0)\n",
    "list_subsets_test[0] = list_subsets_test[0][:,useful_c0]\n",
    "list_features_test[0] = list_features_test[0][useful_c0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=list(range(list_subsets_test[1].shape[1]))\n",
    "useful_c1 = np.delete(b,useless_c1)\n",
    "list_subsets_test[1] = list_subsets_test[1][:,useful_c1]\n",
    "list_features_test[1] = list_features_test[1][useful_c1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=list(range(list_subsets_test[2].shape[1]))\n",
    "useful_c2 = np.delete(c,useless_c2)\n",
    "list_subsets_test[2] = list_subsets_test[2][:,useful_c2]\n",
    "list_features_test[2] = list_features_test[2][useful_c2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(3):\n",
    "    list_subsets_test[idx]= (list_subsets_test[idx] - list_means[idx]) / list_std[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding both test and train dataset according to degrees in best_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(3):\n",
    "    list_subsets[idx] = build_poly(list_subsets[idx],best_degrees[idx],how_many_trig_features[idx])\n",
    "    list_subsets_test[idx] = build_poly(list_subsets_test[idx],best_degrees[idx],how_many_trig_features[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ridge regression model for each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ws = [0]*3\n",
    "final_ws[0],_ = ridge_regression(y_0,list_subsets[0],best_lambdas[0])\n",
    "final_ws[1],_ = ridge_regression(y_1,list_subsets[1],best_lambdas[1])\n",
    "final_ws[2],_ = ridge_regression(y_2_3,list_subsets[2],best_lambdas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing predictions for each test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_0 = predict_ridge(list_subsets_test[0],final_ws[0])\n",
    "prediction_1 = predict_ridge(list_subsets_test[1],final_ws[1])\n",
    "prediction_2 = predict_ridge(list_subsets_test[2],final_ws[2])\n",
    "all_predictions = np.concatenate([prediction_0,prediction_1,prediction_2])\n",
    "all_ids = np.concatenate([ids_0_test,ids_1_test,ids_2_3_test])\n",
    "all_predictions, all_ids = reordering_predictions(all_predictions,all_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(all_ids,all_predictions,['Id','Prediction'],'./output/ridge_regression_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e9c25df0253b19710fd2eabc0119b804c820fc74e8ba938ebab686d76fc4dfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
